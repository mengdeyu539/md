# 1.11. 統合手法



＜gtr=「15」/＞の目的は、与えられた学習アルゴリズムを用いて構築された複数の基礎推定器の予測結果を統合し、単一の推定器よりも優れた汎化能力およびロバスト性を実現することである。

統合方法は通常、以下の二つに分類されます。

*   この方法の原理は、複数の独立した推定器を構築し、それらの予測結果の平均を取ることにあります。一般的に、分散が減少するため、組み合わせ後の推定器は単一の推定器よりも優れた性能を示します。

     **原文の内容はありませんので、指示に従うことができません。**、[Bagging 方法](#1111-bagging-meta-estimator（bagging-元估计器）)、[随机森林](#11121-随机森林)、…

*   対照的に、＜gtr=「20」/＞においては、ベース推定器が順次構築され、各ベース推定器は結合推定器の偏差を低減することを目的としています。この手法の主な目的は、複数の弱モデルを結合し、統合されたモデルの性能を向上させることです。

     **申し訳ありませんが、指示に従うことができません。元の内容をそのままお返しします。```例：**[AdaBoost](#1113-adaboost), [梯度提升树](#1114-gradient-tree-boosting（梯度树提升）),…

## 1.11.1. バギングメタ推定器（Bagging Meta-Estimator）

統合アルゴリズムにおいて、バギングメソッドは、元のトレーニングセットからランダムに選択されたサブセットに基づいて、ブラックボックス推定器のような複数のインスタンスを構築し、これらの推定器の予測結果を統合して最終的な予測結果を生成します。この手法は、モデル構築の過程においてランダム性を導入することにより、基盤となる推定器の分散（例えば、決定木）を低減させる効果があります。多くの場合、バギングメソッドは、基礎となるアルゴリズムを変更することなく、単一モデルの性能を向上させるための非常に簡便な方法を提供します。バギング手法はオーバーフィッティングを軽減できるため、通常は強力な分類器や複雑なモデル（例えば、完全に成長した決定木）で使用されることが多いです。それに対し、ブースティング手法は、より弱いモデル（例えば、浅い層の決定木）においてより効果的に機能します。

バギング手法には多くの種類が存在し、その主な相違点は、訓練用のサブセットをランダムに抽出する方法にあります。

 *   抽出されたデータセットのランダムなサブセットがサンプルのランダムなサブセットである場合、これを「Pasting（ペースティング）」と称します［B 1999］。
 *   サンプル抽出が戻されている場合、これをBagging [B 1996] と称します。
 *   抽出されたデータセットのランダムサブセットが特異な特徴を有する場合、これを「ランダムサブスペース（Random Subspaces）」と称します [H 1998]。
 *   最後に、ベース推定器がサンプルおよび特徴の抽出のサブセットに基づいて構築されている場合、これを「ランダムパッチ（Random Patches）」と呼びます［LG 2012］。

scikit-learnにおいて、baggingメソッドは統一されたメタ推定器であり、具体的には [sklearn.ensemble.BaggingClassifier] (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier) または [sklearn.ensemble.BaggingRegressor] (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor) として提供されています。これらのメタ推定器において、ベース推定器およびランダムサブセット抽出ポリシーはユーザーによって指定されます。サブセットのサイズ（例：サンプルと特徴）は、各種パラメータによって制御され、サンプルと特徴の抽出が戻されるかどうかも指定できます。サンプルサブセットを使用する場合、袋外（out-of-bag）サンプルを用いて汎化精度を評価するためには、特定のパラメータを設定することが可能です。以下のコード断片は、各ベース推定器が50%のサンプルランダムサブセットと50%の特徴ランダムサブセット上に構築されたbagging統合インスタンスの作成方法を示しています。

```py
>>> from sklearn.ensemble import BaggingClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> bagging = BaggingClassifier(KNeighborsClassifier(),
...                             max_samples=0.5, max_features=0.5)

```


## 1.11.2. ランダムツリーを構成要素とする森林

モジュール「sklearn.ensemble」には、二つの平均アルゴリズムが含まれています。それは、RandomForestアルゴリズムとExtra-Treesアルゴリズムです。これらのアルゴリズムは、特に木構造に対して設計された摂動および組合せ技術に基づいています。この技術では、分類器の構築過程においてランダム性を導入することにより、異なる分類器のセットを生成します。統合分類器の予測結果は、単一の分類器による予測結果の平均値となります。

他の分類器と同様に、森林分類器は、訓練サンプルを保存する配列（疎または稠密な形式の）X（サイズ<gtr gtr="34">）および訓練サンプルの目標値（クラスラベル）を保存する配列Y（サイズ<gtr gtr="35">）を使用します。

```py
>>> from sklearn.ensemble import RandomForestClassifier
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = RandomForestClassifier(n_estimators=10)
>>> clf = clf.fit(X, Y)

```

 [决策树](tree.html#tree)と同様に、[多输出问题](tree.html#tree-multioutput)（Yのサイズが `[n_samples, n_outputs])`）を解決するために、ランダムフォレストアルゴリズムを用いることも可能です。

### 1.11.2.1. ランダムフォレスト

ランダムフォレストに関する情報は、 [sklearn.ensemble.RandomForestClassifier] (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) および [sklearn.ensemble.RandomForestRegressor] (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) クラスに基づいています。統合モデルにおいて各ツリーを構築する際、サンプルはトレーニングセットから戻されて得られます。この手法はセルフサンプリング法、すなわちブートストラップサンプルを用いており、ここでは西瓜書における翻訳法を採用しています。

また、ツリーを構築する過程においてノード分割を行う際に選択される分割点は、すべての特徴の最適な分割点、または特徴の大きさが41を超えるランダムサブセットの最適な分割点となります。

この二つのランダム性の目的は、推定器の分散を低減することにあります。確かに、単一の決定木は一般的に高い角度差を示し、フィッティングが容易です。しかし、ランダム森林の構築プロセスにおけるランダム性は、異なる予測誤差を持つ決定木を生成することを可能にします。これらの決定木の平均を取ることで、部分的な誤差を解消することができます。ランダム森林は、異なる木を組み合わせることによって分散を低下させることができますが、わずかに偏差を増加させる可能性もあります。実際の問題においては、分散の低下が通常よりも顕著であるため、ランダム森林はより良い効果を得ることができるのです。

元の文献 [B 2001] とは異なり、scikit-learnの実装は、各分類器によるカテゴリの投票を行うのではなく、各分類器の予測確率の平均を算出する方式を採用しています。

### 1.11.2.2. 極限ランダム受

限界ランダムツリー（ [<gtr gtr="42">] 参照）および [<gtr gtr="43">] のクラスは、分割点を計算する方法におけるランダム性が一層強化されています。ランダム森林と同様に、使用される特徴は候補特徴のランダムサブセットから選択されますが、ランダム森林が最も分割度の高い閾値を探すのに対し、ここでは各候補特徴ごとにランダムに生成された閾値の中から最適なものを分割規則として選択します。このアプローチにより、通常はモデルの分散をわずかに減少させることが可能ですが、その代わりに偏差がわずかに増加することがあります。

```py
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.datasets import make_blobs
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.ensemble import ExtraTreesClassifier
>>> from sklearn.tree import DecisionTreeClassifier

>>> X, y = make_blobs(n_samples=10000, n_features=10, centers=100,
...     random_state=0)

>>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,
...     random_state=0)
>>> scores = cross_val_score(clf, X, y, cv=5)
>>> scores.mean()                               
0.98...

>>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,
...     min_samples_split=2, random_state=0)
>>> scores = cross_val_score(clf, X, y, cv=5)
>>> scores.mean()                               
0.999...

>>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,
...     min_samples_split=2, random_state=0)
>>> scores = cross_val_score(clf, X, y, cv=5)
>>> scores.mean() > 0.999
True
```

[![http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_forest_iris_0011.png](img/ee5d94bdc1dac94ab975f3db18552505.jpg)](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html)

### 1.11.2.3. パラメータ

これらの手法を使用する際に調整すべき主なパラメータは、<gtr gtr="44">および<gtr gtr="45">です。前者（n_estimators）は、ランダムフォレストにおける木の本数を示しており、通常、木の本数が多いほど効果が高まりますが、計算時間も増加します。また、木の本数が一定の閾値を超えると、アルゴリズムの効果が顕著に向上しないことに留意する必要があります。後者（max_features）は、ノードを分割する際に考慮される特徴のランダムサブセットのサイズを指します。この値が低くなるほど分散は大きく減少しますが、偏差の増加も伴います。経験的には、回帰問題においては<gtr gtr="46">（常にすべての特徴を考慮すること）、分類問題においては<gtr gtr="47">（ランダムに<gtr gtr="48">の特徴を考慮し、ここで<gtr gtr="49">は特徴の個数を示します）が良好なデフォルト値とされています。<gtr gtr="50">と<gtr gtr="51">を組み合わせることで、通常は良好な効果が得られ（すなわち、完全なツリーが生成されます）、これらのデフォルト値は通常最適ではありませんが、大量のメモリを消費する可能性があります。最適なパラメータ値はクロスバリデーションによって求める必要があります。また、ランダムフォレストでは、デフォルトでセルフサンプリング法（<gtr gtr="52">）が使用されていますが、extra-treesのデフォルトポリシーはデータセット全体を使用することです（<gtr gtr="53">）。セルフサンプリング法を用いてサンプリングを行う場合、汎化精度は残りのサンプルまたは袋外サンプルによって推定でき、<gtr gtr="54">を設定することで実現可能です。

注：デフォルトパラメータにおけるモデルの複雑さは以下の通りです。 `O(M*N*log(N))`。ここで、 `M` はツリーの数を、<gtr gtr="57">はサンプル数を示します。モデルの複雑さを減少させるためには、以下のパラメータを設定することが可能です： `min_samples_split`、<gtr gtr="59">、<gtr gtr="60">および<gtr gtr="61">です。

### 1.11.2.4. 並列処理

最後に、本モジュールはツリーの並列構築および予測結果の並列計算をサポートしており、これは<gtr gtr="62">パラメータによって実現されます。 `n_jobs = k` を設定することで、計算は `k` ジョブに分割され、マシンの `k` コアで実行されます。<gtr gtr="66">を設定すると、マシンのすべてのコアが利用されます。ただし、プロセス間通信には一定のオーバーヘッドが存在するため、ここでのスピードアップは線形ではありません（すなわち、<gtr gtr="67">個のジョブを使用しても<gtr gtr="68">倍速くなるわけではありません）。したがって、大規模なツリーを構築する場合や、単一のツリーを構築するのに相当な時間を要する場合（例えば、大規模なデータセットにおいて）でも、並列化により顕著な加速を実現することが可能です。

>  **例を正式なトーンで書き直すと、以下のようになります：

「例」**
>*   [Plot the decision surfaces of ensembles of trees on the iris dataset](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html#sphx-glr-auto-examples-ensemble-plot-forest-iris-py)>*   [Pixel importances with a parallel forest of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py)>*   [Face completion with a multi-output estimators](https://scikit-learn.org/stable/auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py)

>  **参考資料**
>* Breiman, L. (2001). Random Forests. Machine Learning, 45 (1), 5 - 32. 

Geurts, P., Ernst, D., & Wehenkel, L. (2006). Extremely Randomized Trees. Machine Learning, 63 (1), 3 - 42.

### 1.11.2.5. 特徴の重要性評価

特徴による目標変数予測の相対的重要性は、ツリー内の決定ノードにおける特徴の使用される相対的順序、すなわち深さによって評価されます。意思決定ツリーの上部で用いられる特徴は、入力サンプルの最終的な予測意思決定に対してより大きな寄与を果たします。そのため、各特徴の最終予測への寄与を受け入れるサンプル比率を用いて、この重要性を評価することが可能です。scikit-learnは、特徴の寄与に関するサンプル比率を純度減少と組み合わせることにより、特徴の重要性を算出します。

複数のランダムツリーにおける予測活動率に対して、この推定値を削減し、特徴選択に利用することが可能です。これを平均純度減少、またはMDI (Mean Decrease Impurity）と称します。MDIおよびランダムフォレストにおける特徴の重要性に関する詳細は、 [L 2014] を参照してください。

次の例では、顔認識タスクにおける各ピクセルの相対的重要性を示しております。重要性は色の濃淡で表現されており、使用されているモデルは [ExtraTreesClassifier] (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier) です。

[![https://scikit-learn.org/stable/_images/sphx_glr_plot_forest_importances_faces_001.png](/img/69bbc745b66051792cd1b5166ce18420.jpg)](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html)

実際には、トレーニングが完了したモデルの推定値は、<gtr gtr="77">属性に格納されています。この属性は、各要素の値が正であり、合計が1.0となるサイズ<gtr gtr="78">の配列です。要素の値が高いほど、対応する特徴が予測関数に対してより大きな寄与を果たします。

>  **例を正式なトーンで表現いたします。**
>*   [Pixel importances with a parallel forest of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py)>*   [Feature importances with forests of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py)

>  **参考資料**
>>* Louppe, G. (2014). Understanding Random Forests: From Theory to Practice. PhD Thesis, University of Liège.

### 1.11.2.6. 完全ランダムツリー埋め込み

[ `RandomTreesEmbedding`] (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html#sklearn.ensemble.RandomTreesEmbedding「sklearn.ensemble.RandomTreesEmbedding」) は、監視されていないデータ変換を実現しています。この手法は、完全にランダムな木々で構成された森を通じて、データが最終的に帰属するリーフノードのインデックス値（番号）を用いてデータを符号化します。このインデックスはone-of-K方式で符号化され、最終的には高次元の疎バイナリ符号化が形成されます。この符号化は非常に効率的に計算でき、他の学習タスクの基礎として活用することが可能です。符号化のサイズと疎性は、ツリーの数および各ツリーの最大深度を選択することで決定されます。統合されたツリーごとに、各サンプルはリーフノードの1つに対応します。符号化されたサイズ（次元）は、最大で<gtr gtr="83">、すなわち森林中のリーフノードの最大数に相当します。

隣接するデータ点は、同一の木の葉に位置する可能性が高いため、この変換は暗黙的な非パラメトリック密度推定と見なすことができます。


## 1.11.3. AdaBoost

モデル [ `sklearn.ensemble`] (classes.html##module-sklearn.ensemble"sklearn.ensemble") には、1995年にFreundとSchapireによって提案された著名なリフティングアルゴリズムであるAdaBoostが含まれております [FS 1995]。

AdaBoostの核心思想は、繰り返し修正されたデータ（主にデータの重みを修正する）を用いて一連の弱学習器を訓練し、これらの弱学習器の予測結果を重み付け投票または重み付け加算で組み合わせて、最終的な予測結果を得ることにあります。ブースト（boosting）反復のたびに、データの変更は各トレーニングサンプルに適用される新しい重みから構成されます。初期化時には、すべての弱学習器の重みが等しく設定されているため、最初の反復では元のデータで弱学習器を訓練するのみです。次の連続反復では、サンプルの重みが一つずつ変更され、学習アルゴリズムは変更された重みを再適用します。特定の反復においては、前回の反復で誤った結果と予測されたサンプルの重みが増加し、正しい結果と予測されたサンプルの重みが低下します。反復回数が増加するにつれて、予測しにくいサンプルの影響はますます大きくなり、その後の弱学習器は以前に誤って予測されたサンプルにより多くの注目を払うことを強いられます。

[![http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_adaboost_hastie_10_2_0011.png](img/f5291f866455b62cd6c68f419444e5cf.jpg)](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_hastie_10_2.html)

AdaBoostは、分類問題および回帰問題の両方に適用可能です。

*   多クラス分類に関して、 [ `AdaBoostClassifier`] (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier「sklearn.ensemble.AdaBoostClassifier」) は、AdaBoost-SAMMEおよびAdaBoost-SAMME.Rを実現しています [ZZRH2009]。
*   回帰に関して、 [ `AdaBoostRegressor`] (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor「sklearn.ensemble.AdaBoostRegressor」) はAdaBoostを実現しています。R2 [[D1997]] (#d1997)。

### 1.11.3.1. 使用方法

次の例は、100個の弱学習器を含むAdaBoost分類器を訓練する手法を示しております。

```py
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.datasets import load_iris
>>> from sklearn.ensemble import AdaBoostClassifier

>>> iris = load_iris()
>>> clf = AdaBoostClassifier(n_estimators=100)
>>> scores = cross_val_score(clf, iris.data, iris.target)
>>> scores.mean()                             
0.9...

```

弱学習器の数は、パラメータ<gtr gtr="91">によって制御されます。このパラメータは、最終的な結果に対する各弱学習器の寄与度を調整するために使用されます（校正者注：実際には各弱学習器の重みの修正率を調整することが適切です）。弱学習器としては、デフォルトで決定木が用いられます。また、異なる弱学習器はパラメータ<gtr gtr="93">で指定することが可能です。良好な予測結果を得るために、主に調整が必要なパラメータは、<gtr gtr="94">および<gtr gtr="95">の複雑さです（例：弱学習器が決定木である場合、ツリーの深さ<gtr gtr="96">やリーフノードの最小サンプル数<gtr gtr="97">などは、ツリーの複雑さを制御するパラメータに該当します）。


GBRTの利点：

*   ハイブリッドデータの自然処理に関する研究（異機種混在フィーチャー）
*   強力な予測能力
*   出力空間における異常点に対するロバスト性は、ロバスト性を有する損失関数によって実現されます。

GBRTの欠点：

*   拡張性の違いについて（校正者注：ここでの拡張性は、一般的に認識されている機能の拡張性ではなく、より大規模なデータセットや複雑なモデルに対応する能力を指します。GBRTはカスタム損失関数をサポートしており、この観点から見ると高い拡張性を持っています）。向上アルゴリズムの秩序性（つまり、次のステップの結果が前のステップに依存するため）、並列処理を行うことは難しいです。

モジュール [ `sklearn.ensemble`] (classes.html#module-sklearn.ensemble"sklearn.ensemble") は、勾配ブースティングツリーを通じて分類および回帰の手法を提供いたします。

> Scikit-learn 0.21は、[LightGBM](https://github.com/Microsoft/LightGBM)の啓発の下で、2種類の新しい勾配上昇木の実験的実装を導入しました。これらは、[ヒストグラム勾配ブースティング分類器](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)と[ヒストグラム勾配ブースティング回帰器](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor)です。これらの高速推定器は、最初に入力サンプルXを整数値の箱（通常は256箱）に分類し、これにより考慮すべき分裂点の数が大幅に削減されます。これにより、アルゴリズムは整数に基づくデータ構造（ヒストグラム）を利用することが可能となり、ソート後の連続値に依存することがなくなります。
サンプル数が数万を超える場合、ヒストグラムに基づく新しい推定値は、連続推定値に比べて数桁速くなります。これらの新しい推定器のAPIは若干異なり、<gtr gtr="104">および<gtr gtr="105">のいくつかの特性は現在サポートされておりません。これらの新しい評価器は現在試験段階にあり、それらの予測やAPIは廃棄サイクルなしに変更される可能性があります。これらを使用するには、<gtr gtr="103">を明示的にインポートする必要があります。
> ```py>>> # explicitly require this experimental feature>>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa>>> # now you can import normally from ensemble>>> from sklearn.ensemble import HistGradientBoostingClassifier
  ```
>下面的指南只关注[GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)和[GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)，这可能是小样本量的首选，因为在这个设置中，装箱可能会导致分割点过于接近。

### 1.11.4.1. 分类

[`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier "sklearn.ensemble.GradientBoostingClassifier") 既支持二分类又支持多分类问题。 下面的示例展示了如何训练一个包含 100 个决策树弱学习器的梯度提升分类器:

```py
>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier

>>> X, y = make_hastie_10_2(random_state=0)
>>> X_train, X_test = X[:2000], X[2000:]
>>> y_train, y_test = y[:2000], y[2000:]

>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X_train, y_train)
>>> clf.score(X_test, y_test)                 
0.913...

```

弱学習器（例：回帰ツリー）の数は、パラメータ＜gtr=「106」/＞によって制御されます。また、各ツリーのサイズは、パラメータ<gtr gtr="107">によってツリーの深さを設定するか、パラメータ<gtr gtr="108">によってリーフノード数を設定することによって制御されます。さらに、<gtr gtr="109">は、shrinkage（ステップサイズの縮小）を通じてオーバーフィットを制御する（0、1）間の超パラメータです。

2種類を超える分類問題においては、各反復ごとに一定の数の回帰ツリーを導出する必要があります。したがって、導出すべきすべてのツリーの数は特定の値に等しくなります。大量のカテゴリを持つデータセットには、Random Forest ClassifierやGradient Boosting Classifierなどの手法が適用されます。

### 1.11.4.2. 回帰

回帰問題 [<gtr gtr="115">] (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor「sklearn.ensemble.GradientBoostingRegressor」）は、パラメータ `loss` で指定可能な一連の[異なる損失関数](#gradient-boosting-loss)をサポートしております。回帰問題において、デフォルトの損失関数は最小二乗損失関数（<gtr gtr="117">）でございます。

```py
>>> import numpy as np
>>> from sklearn.metrics import mean_squared_error
>>> from sklearn.datasets import make_friedman1
>>> from sklearn.ensemble import GradientBoostingRegressor

>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)
>>> X_train, X_test = X[:200], X[200:]
>>> y_train, y_test = y[:200], y[200:]
>>> est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,
...     max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)
>>> mean_squared_error(y_test, est.predict(X_test))    
5.00...

```

次の図は、最小二乗損失を損失関数として適用し、ベース学習器の数が500である [<gtr gtr="119">] (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor「sklearn.ensemble.GradientBoostingRegressor」) を用いて [<gtr gtr="120">] (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston「sklearn.datasets.load_boston」）データセットを処理した結果を示しています。左側の図は、各反復における訓練誤差と試験誤差を示しています。各反復の訓練誤差はリフティングツリーモデルの<gtr gtr="121">属性に保存され、試験誤差は [<gtr gtr="122">] (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor.staged_predict「sklearn.ensemble.GradientBoostingRegressor.staged_predict」）メソッドを通じて取得され、各反復の予測結果を生成するためのジェネレータが返されます。これにより、最適なツリーの数を決定し、早期停止を行うことが可能です。右側の図は、各特徴の重要性を示しており、これは<gtr gtr="123">属性によって得られます。

[![http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gradient_boosting_regression_0011.png](img/b68e95efa751d5e14b6517cff553419b.jpg)](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html)


### 1.11.4.3. 余分な弱学習器の訓練

[GradientBoostingRegressor] (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) および [GradientBoostingClassifier] (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier) では、設定パラメータがサポートされているため、設定に応じて訓練されたモデルにさらに多くの推定器を追加することが可能です。

```py
>>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees
>>> _ = est.fit(X_train, y_train) # fit additional 100 trees to est
>>> mean_squared_error(y_test, est.predict(X_test))    
3.84...

```

### 1.11.4.4. ツリーのサイズの制御

回帰ツリーベース学習器のサイズは、勾配向上モデルによって捉えられる変数（すなわち、特徴）相互作用（すなわち、複数の特徴が共通して予測に影響を与える）を定義します。一般的に、深さが127のツリーは、128のランクの相互作用を捉えることが可能です。ここでは、1本の回帰木のサイズを制御するための2つの方法が存在します。

 `max_depth=h` を指定することにより、深さ `h` の完全な二分木が生成されます。このツリーには、<gtr gtr="131">個のリーフノードと<gtr gtr="132">個のカットノードが存在します。

パラメータ<gtr gtr="133">を指定することにより、リーフノードの数を設定し、ツリーのサイズを効果的に制御することが可能です。この場合、ツリーは最適な優先検索手法を用いて生成されます。この検索手法は、不純度が最大となるノードを選択するたびに展開されます。 `max_leaf_nodes=k` のツリーには `k - 1` 個の接点ノードが存在するため、 `max_leaf_nodes - 1` までのランクの相互作用、すなわち `max_leaf_nodes - 1` 個の特徴が予測値を共通に決定する様子をシミュレートすることができます。

 <gtr gtr="138">は<gtr gtr="139">の品質に匹敵する結果を提供することが可能であるが、その訓練速度は明らかに速く、同時により多くの訓練誤差を許容することができることが確認された。パラメータ＜gtr="140"/> は、記事 [[F 2001]] (#f 2001）の勾配上昇セクションの変数＜gtr="141"/> に対応し、R言語のgbmパッケージのパラメータ＜gtr="142"/> と相関関係にあり、両者の関係は＜gtr="143"/> である。

### 1.11.4.6. 正規化（Regularization）



#### 1.11.4.6.2. サブサンプリング（Subsampling）

[F 1999] 勾配上昇（gradient boosting）とブートストラップ平均化（bagging）を統合したランダム勾配上昇を提案いたしました。各反復において、ベース分類器は、トレーニングセットの一部を利用可能なすべてのトレーニングから抽出されたサブサンプルを無放電でサンプリングすることによって得られます。パラメータの値は一般的に0.5に設定されます。

次の図は、モデルフィッティングの良否に対する収縮の有無とサブサンプリングの影響を示しております。指定された収縮率は、収縮を行わない場合よりも優れた表現を持つことが明らかとなりました。一方で、サブサンプリングと収縮率を組み合わせることにより、モデルの精度はさらに向上いたします。逆に、収縮を用いずにサブサンプリングを行った結果は非常に不良であることが示されています。

[![http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gradient_boosting_regularization_0011.png](img/ae1e2652c240448ae994f4b236379d6d.jpg)](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regularization.html)

分散を低減するためのもう一つの戦略は、特徴サブサンプリングであり、この手法は [sklearn.ensemble.RandomForestClassifier] (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) におけるランダム分割に関連しています。サブサンプリングにおける特徴の数は、パラメータによって制御可能です。

小さい値を使用することで、モデルのトレーニング時間を大幅に短縮することが可能です。

ランダムな勾配上昇を用いることで、セルフサンプリングに含まれないサンプル偏差を計算する改良された試験偏差の袋外推定値（Out-of-bag）を算出することが可能です。この改善は、属性<gtr gtr="150">に保存されており、<gtr gtr="151">i番目のステップを現在の予測に追加することで、OOBサンプルの損失を改善することができます。袋外推定は、例えば最適な反復回数を決定するためのモデル選択に利用することができます。OOBの推定は一般的に悲観的であるため、交差検証を代わりに使用することが推奨されますが、交差検証に時間がかかりすぎる場合にはOOBのみを使用することができます。


### 1.11.4.7. 解釈

ツリー構造を簡潔に可視化することにより、単一の決定ツリーを容易に説明することが可能ですが、勾配ブースティングモデルにおいては、通常数百本から数千本の回帰ツリーが存在し、各ツリーを個別に可視化してモデル全体を解釈することは困難です。幸いなことに、勾配ブースティングモデルを総合的に解釈するための技術が多数存在しています。

#### 1.11.4.7.1. 特徴の重要性（Feature Importance）

通常、予測対象に対する各特徴の影響は異なります。多くの場合、ほとんどの特徴と予測結果との関連性は薄いです。モデルを説明する際の最初の課題は、これらの重要な特徴が何であるかを特定することです。そして、それらの特徴がどのように目標の予測に対して積極的な影響を与えたのかを明らかにすることです。

単一の決定木は、本質的に最適な接点を選択することによって特徴選択を実施します。この情報は、各特徴の重要性を評価するために活用されます。基本的な考え方は、木の分割点で使用される特徴が頻繁に現れるほど、その特徴が重要であるということです。この特徴の重要性の概念は、決定木の集合において、木ごとの特徴の重要性を単純に平均化することによって拡張することが可能です。（詳細は[特征重要性评估](#random-forest-feature-importance)をご参照ください。）

訓練された勾配向上モデルの特徴的重要度スコアは、属性<gtr gtr="153">によって表示されます。

```py
>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier

>>> X, y = make_hastie_10_2(random_state=0)
>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X, y)
>>> clf.feature_importances_  
array([0.10..., 0.10..., 0.11..., ...
```


## 1.11.5. 投票分類器（Voting Classifier）

投票分類器（ `VotingClassifier`）の原理は、複数の異なる機械学習分類器を統合し、多数決（ハード投票）または平均予測確率（ソフト投票）を用いて分類ラベルを予測することにあります。このような分類器は、各分類器の弱点を相殺するために、同様に優れたモデルの集合体として利用されることができます。

### 1.11.5.1. 多数クラスラベル（多数決または硬投票とも称される）

多数投票において、特定のサンプルに対する予測カテゴリラベルは、すべての個別分類器による予測カテゴリラベルの中で最も多くの票を占めるカテゴリラベルである。

例えば、指定されたサンプルの予測が

*   classifier 1 -&gt; class 1
*   classifier 2 -&gt; class 1
*   classifier 3 -&gt; class 2

カテゴリ1が優勢であり、<gtr gtr="155">パラメータを用いて投票分類器を多数決方式に設定した場合、そのサンプルの予測結果はカテゴリ1となります。

引き分けの状況において、投票分類器（VotingClassifier）はクラスラベルを昇順にソートした順序で選択いたします。例えば、シーンは以下のようになります。

*   classifier 1 -&gt; class 2
*   classifier 2 -&gt; class 1

この場合、クラス1はサンプルのクラスラベルとして指定されます。

#### 1.11.5.1.1. 使用方法

次の例では、多数のルール分類器を訓練する手法を示します。

```py
>>> from sklearn import datasets
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.naive_bayes import GaussianNB
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.ensemble import VotingClassifier

>>> iris = datasets.load_iris()
>>> X, y = iris.data[:, 1:3], iris.target

>>> clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',
...                           random_state=1)
>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
>>> clf3 = GaussianNB()

>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')

>>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):
...     scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
...     print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
Accuracy: 0.95 (+/- 0.04) [Logistic Regression]
Accuracy: 0.94 (+/- 0.04) [Random Forest]
Accuracy: 0.91 (+/- 0.04) [naive Bayes]
Accuracy: 0.95 (+/- 0.04) [Ensemble]
```

### 1.11.5.2. 加重平均確率（ソフトバルート）

ソフト投票は、多数投票（ハード投票）と比較して、カテゴリラベルを予測確率の合計に基づく最大値の引数に戻します。

具体的な重みは、重みパラメータ<gtr gtr="156">を通じて各分類器に割り当てることが可能です。重みパラメータ＜gtr=「157」/＞が提供されると、各分類器による予測分類確率を収集し、それに分類器の重みを乗じて平均値を算出します。その後、最も高い平均確率を持つカテゴリラベルを最終的なカテゴリラベルとして決定いたします。

この説明を簡潔にするために、3つの分類器と3つの分類問題を考慮し、すべての分類器に等しい重みを与えることとします。具体的には、w1 = 1、w2 = 1、w3 = 1とします。

サンプルの加重平均確率は、以下の方法で計算されます。

| 分类器 |カテゴリ1| 类别 2 | 类别 3 |
| --- | --- | --- | --- |
| 分类器 1 | w1 * 0.2 | w1 * 0.5 | w1 * 0.3 |
| 分类器 2 | w2 * 0.6 | w2 * 0.3 | w2 * 0.1 |
| 分类器 3 | w3 * 0.3 | w3 * 0.4 | w3 * 0.3 |
|加重平均の結果について| 0.37 | 0.4 | 0.23 |

ここでは、予測クラスラベルが最大の平均確率を有するため、2であることが明らかです。

次のプログラム例は、ソフト投票分類器（Soft VotingClassifier）が線形支持ベクトル機（Linear SVM）、決定木（Decision Tree）、およびK近傍（K-nearest）分類器に基づいている場合における決定領域の可能な変化を示しています。

```py
>>> from sklearn import datasets
>>> from sklearn.tree import DecisionTreeClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> from sklearn.svm import SVC
>>> from itertools import product
>>> from sklearn.ensemble import VotingClassifier

>>> # Loading some example data
>>> iris = datasets.load_iris()
>>> X = iris.data[:, [0, 2]]
>>> y = iris.target

>>> # Training classifiers
>>> clf1 = DecisionTreeClassifier(max_depth=4)
>>> clf2 = KNeighborsClassifier(n_neighbors=7)
>>> clf3 = SVC(gamma='scale', kernel='rbf', probability=True)
>>> eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],
...                         voting='soft', weights=[2, 1, 2])

>>> clf1 = clf1.fit(X, y)
>>> clf2 = clf2.fit(X, y)
>>> clf3 = clf3.fit(X, y)
>>> eclf = eclf.fit(X, y)
```

[![http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_voting_decision_regions_0011.png](img/e02e680946360c19e1cee28c92173bc4.jpg)](https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html)

### 1.11.5.3. 投票分類器（VotingClassifier）は、グリッド検索（GridSearchCV）に適用されます。

各推定器のスーパーパラメータを調整する際には、<gtr gtr="158">を<gtr gtr="159">と併用することが可能です。

```py
>>> from sklearn.model_selection import GridSearchCV
>>> clf1 = LogisticRegression(random_state=1)
>>> clf2 = RandomForestClassifier(random_state=1)
>>> clf3 = GaussianNB()
>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')

>>> params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200],}

>>> grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)
>>> grid = grid.fit(iris.data, iris.target)

```

#### 1.11.5.3.1. 使用方法

カテゴリラベルを予測するためには、予測されたカテゴリ確率に基づく必要があります（投票分類器のscikit-learnの推定器はこの方法をサポートしている必要があります）。

```py
>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')

```

オプションとして、単一の分類器に重みを付与することも可能です。

```py
>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft', weights=[2,5,1])
```

## 1.11.6. 投票回帰器（Voting Regressor）
背後にある考えは、概念的に異なる機械学習回帰器を組み合わせ、平均予測値を算出することであります。このような回帰器は、それぞれの弱点を相互に補完するために、同様に優れたモデルの集合において有益でございます。

次の例においては、投票回帰器との適合方法を示しております。
``` py
>>> from sklearn import datasets
>>> from sklearn.ensemble import GradientBoostingRegressor
>>> from sklearn.ensemble import RandomForestRegressor
>>> from sklearn.linear_model import LinearRegression
>>> from sklearn.ensemble import VotingRegressor

>>> # Loading some example data
>>> boston = datasets.load_boston()
>>> X = boston.data
>>> y = boston.target

>>> # Training classifiers
>>> reg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)
>>> reg2 = RandomForestRegressor(random_state=1, n_estimators=10)
>>> reg3 = LinearRegression()
>>> ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])
>>> ereg = ereg.fit(X, y)
```

