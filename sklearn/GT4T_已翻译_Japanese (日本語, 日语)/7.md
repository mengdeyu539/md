# 1.6. 近隣地域


[ `sklearn.neighbors`] (https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors) は、近隣ベースの監視なし学習及び監視学習の機能を提供します。監視されていない最近傍は、多くの他の学習手法の基盤を成しており、特に多様体学習とスペクトルクラスタリングにおいて重要です。近隣ベースの監視学習には、主に2種類の手法があります。<gtr gtr="7">（分類）は、離散的なラベルを持つデータに適用され、<gtr gtr="8">（回帰）は、連続的なラベルを持つデータに対象を絞っています。

最近傍法の背後にある原理は、訓練サンプルから新しい点に最も近い所定数の点を特定し、それに基づいてラベルを予測することにあります。これらの点の数は、ユーザーがカスタマイズ可能な定数（K-最近接学習）である場合もあれば、異なる点の局所的な密度（半径に基づく最近接学習）に基づいて決定されることもあります。距離は通常、任意のメトリックによって測定可能ですが、標準的なユークリッド距離が最も一般的な選択肢となります。ネイバーベースメソッドは、すべての訓練データを容易に記憶するため、機械学習メソッドの一種と見なされます。

最近の近隣アルゴリズムは、手書きのデジタルデータや衛星画像のシーンを含む多様な分類および回帰問題において、成功を収めております。この手法は非パラメトリックなアプローチであり、意思決定境界が非常に不規則な分類シナリオにも効果的に適用されることが多いです。

[ `sklearn.neighbors`] (https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors) は、Numpy配列または<gtr gtr="13">行列を入力として処理することができます。密行列の場合、ほとんどの距離測定方法がサポートされています。一方、疎行列の場合は、任意のMinkowskiメトリックの検索がサポートされています。

多くの学習経路や方法は、最隣接を中心に構築されています。一例として、<gtr gtr="14">および<gtr gtr="15">の章で詳述されています。

## 1.6.1. 無監督最隣接法

「sklearn.neighbors.NearestNeighbors」（最近傍）は、監督なしの最近傍学習を実現しています。このモジュールは、3つの異なる最近傍アルゴリズムに統合されたインターフェースを提供します。具体的には、 [sklearn.neighbors.BallTree] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree)、 [sklearn.neighbors.KDTree] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree)、さらに、 [sklearn.metrics.pairwise] (classes.html#module-sklearn.metrics.pairwise) に基づくブートフォースアルゴリズムが含まれています。アルゴリズムの選択は、キーワードによって制御され、指定されたいずれかの方法でなければなりません。デフォルト値に設定することで、アルゴリズムはトレーニングデータから最適な方法を決定しようとします。各オプションの長所と短所については、関連する資料を参照してください。

>  **警告を発する**
>
> 最隣接アルゴリズムにおいて、近隣<gtr gtr="25">と近隣<gtr gtr="26">が同一の距離を持ちながら異なるラベルを有する場合、得られる結果はトレーニングデータの順序に依存します。

### 1.6.1.1. 最も近接するものを特定する

2つのデータセットの最隣接点を特定する簡易なタスクを遂行するためには、 [<gtr gtr="27">] (https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors）に記載されている監視なしアルゴリズムを利用することが推奨されます。

```py
>>> from sklearn.neighbors import NearestNeighbors
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)
>>> distances, indices = nbrs.kneighbors(X)
>>> indices                                           
array([[0, 1],
 [1, 0],
 [2, 1],
 [3, 4],
 [4, 3],
 [5, 4]]...)
>>> distances
array([[0.        , 1.        ],
       [0.        , 1.        ],
       [0.        , 1.41421356],
       [0.        , 1.        ],
       [0.        , 1.        ],
       [0.        , 1.41421356]])

```

クエリ・セットはトレーニング・セットと一致しているため、各ポイントの最も近接した点はそのポイント自身であり、距離は0となります。

接続点間の接続状況を特定するための疎マップを効率的に生成することが可能です。

```py
>>> nbrs.kneighbors_graph(X).toarray()
array([[ 1.,  1.,  0.,  0.,  0.,  0.],
 [ 1.,  1.,  0.,  0.,  0.,  0.],
 [ 0.,  1.,  1.,  0.,  0.,  0.],
 [ 0.,  0.,  0.,  1.,  1.,  0.],
 [ 0.,  0.,  0.,  1.,  1.,  0.],
 [ 0.,  0.,  0.,  0.,  1.,  1.]])

```

私たちのデータセットは構造化されており、インデックス順の隣接点がパラメータ空間に隣接しているため、K-nearest neighbors (K-近隣）に近いブロック対角行列を生成しています。このような疎図は、さまざまな利用点間の空間関係を監視なしに学習する際に有用です。特に、 [Isomap] (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap)、 [Locally Linear Embedding] (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding)、および [Spectral Clustering] (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering) において重要な役割を果たします。

### 1.6.1.2. KDTreeおよびBallTreeクラス

また、 [sklearn.neighbors.KDTree] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html) または [sklearn.neighbors.BallTree] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html) を探しに来ました。これらは、上記で使用した [sklearn.neighbors.NearestNeighbors] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html) クラスに含まれる機能です。 [KDTree] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html) と [BallTree] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html) は同じインターフェースを持ち、ここでは、 [sklearn.neighbors.KDTree] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html) の例を示します。

```py
>>> from sklearn.neighbors import KDTree
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> kdt = KDTree(X, leaf_size=30, metric='euclidean')
>>> kdt.query(X, k=2, return_distance=False)          
array([[0, 1],
 [1, 0],
 [2, 1],
 [3, 4],
 [4, 3],
 [5, 4]]...)

```

近隣検索のオプションに関する詳細については、さまざまな距離メジャーの説明やポリシーに関して、 [sklearn.neighbors.KDTree] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree) および [sklearn.neighbors.BallTree] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) のクラスドキュメントをご参照ください。使用可能な距離メジャーの一覧については、 [sklearn.neighbors.DistanceMetric] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric) のクラスをご覧ください。

## 1.6.2. 最隣接分類法

最近傍分類は、<gtr gtr="40">または<gtr gtr="41">において、一般化された内部モデルの構築を行うのではなく、トレーニングデータのインスタンスを容易に保存する手法です。分類は、各ポイントに対して最も近い隣接点の単純な多数決に基づいて計算されます。クエリーポイントのデータ型は、その最も近い隣接点の中で最も代表的なデータ型によって決定されます。


scikit-learnは、各クエリポイントに基づく指定された数の最近傍実装を持つ2種類の最近傍分類器を実装しています。ここで、指定された整数値はユーザーによって設定されます。具体的には、 [sklearn.neighbors.RadiusNeighborsClassifier] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier) は、各クエリポイントの固定半径内に存在する近隣の数に基づいて実装されており、ここでの半径はユーザーによって指定される浮動小数点数です。

隣人分類は、技術の一つとして比較的一般的に使用されている手法です。最適な値の選択は、データに大きく依存します。通常、大きな値はノイズの影響を抑制しますが、分類の限界を明らかにすることはできません。

データが不均一にサンプリングされている場合、 [<gtr gtr="50">] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier「sklearn.neighbors.RadiusNeighborsClassifier」）に基づく半径近傍分類器がより適切な選択となる可能性があります。ユーザーは、疎な隣接点が中心の点よりも少ない最も近い隣接点を使用して分類を行うために、固定された半径を指定することができます。ただし、高次元のパラメータ空間においては、この手法は「次元の呪い」と呼ばれる現象により、あまり効果的ではありません。

基本的な最近傍分類は、統一された重みを用いて実施されます。クエリポイントに対して割り当てられる値は、最近傍の簡易的な多数決に基づいて算出されます。特定の環境においては、近隣諸国に対して重み付けを行うことが望ましく、これにより近隣諸国がより適切にフィットするようになります。これを実現するためには、キーワードを使用することが可能です。デフォルトの設定では、近隣ごとに均一な重みが割り当てられます。一方で、割り当てられるウェイトはクエリポイントからの距離に反比例します。また、重みを計算するための距離関数をカスタマイズすることも選択肢として存在します。

**![classification_1](img/1a91bab921cf39f58a522ed15f475235.jpg) ![classification_2](img/ae484baf10384efcf4d993631f4641e7.jpg)**

>  **例を正式なトーンで書き直すと、以下のようになります：

例示**
>>*   ＜gtr=「55」/>：最隣接法を用いた分類の一例。

## 1.6.3. 最近の臨界期

最近傍回帰は、データラベルが離散変数ではなく連続変数である場合に適用されます。クエリーポイントに割り当てられるラベルは、その最も近い隣接ラベルの平均値に基づいて算出されます。

scikit-learnは、2種類の異なる最近傍回帰手法を実装しています。まず、 [KNeighborsRegressor] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor) は、各クエリポイントに対して指定された数の最近傍を利用します。この最近傍の数は、ユーザーが指定する整数値です。次に、 [RadiusNeighborsRegressor] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor) は、各クエリポイントの周囲にある固定の半径内に存在する隣接ポイントの数に基づいています。この半径の値は、ユーザーが指定する浮動小数点数です。

基本的な最近傍回帰においては、統一された重みが適用されます。これは、ローカル近傍内の各隣接点がクエリ点の分類に対して均等に寄与することを意味します。しかしながら、特定の環境においては、近傍点が遠方点よりも回帰に対してより大きな寄与を行うよう、隣接点に重みを付与することが有利である場合があります。この重み付けは、 `weights` キーワードを用いることで実現可能です。デフォルトでは、 `weights = 'uniform'` はすべてのポイントに同等の重みを割り当てます。一方で、<gtr gtr="64">によって割り当てられる重みはクエリポイントの距離に反比例します。また、重みを計算するために使用する距離関数をカスタマイズすることも可能です。

![http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_regression_0011.png](img/207e92cfc624372bc9c72a160c02114f.jpg)





## 1.6.4. 最隣接アルゴリズム

### 1.6.4.1. 暴力的計算について

最近傍の高速計算は、機械学習における活発な研究分野の一つでございます。最も基本的な近隣検索を実現するためには、データセット内のすべての対点間の距離を計算する必要があります。次元がDでサンプル数がNの場合、この手法の計算複雑度はO (N^2) となります。小規模なデータサンプルに対しては、効率的な暴力的近隣検索が非常に競争力がありますが、サンプル数が増加するにつれて、この暴力的手法は急速に現実的でなくなります。Scikit-learnのクラスにおいて、暴力的近隣検索はキーワードによって指定され、関連するルーチンは「sklearn.metrics.pairwise」で計算されます。

### 1.6.4.2. K-Dツリー

非効率的な暴力計算方法を解決するために、木構造に基づく多数のデータ構造が開発されています。これらの構造は、サンプルの重合距離情報を効果的に符号化することによって、必要な距離計算量を削減することを目的としています。基本的な考え方は、ある点から別の点までの距離が非常に遠い場合、他の点同士の距離は非常に近く、また別の点とその点との距離は非常に遠いということです。このようにして、近隣検索の計算コストを削減することが可能となります。計算量はO [DN log (N)]以下となり、これは暴力的検索における大規模なサンプル数に対する顕著な改善を示しています。

初期の集約情報利用方法として、2次元および3次元のデータ構造を任意の次元に拡張する手法が挙げられます。KDツリーは、データ軸に沿ってパラメータ空間を再帰的に分割し、データ点が埋め込まれたネストされた異方性領域に分割する二分木構造です。この構造は、データ軸に沿ってパーティションを実行するだけで、次元距離を計算する必要がないため、非常に高速に構築されます。構築が完了すると、クエリポイントの最近接距離の計算の複雑さはわずか O [log (N)] となります。KDツリーの手法は低次元における近隣検索において非常に迅速ですが、次元数が大きく成長すると効率が低下します。これはいわゆる「次元の呪い」の一例です。scikit-learnにおいては、KDツリーによる近隣検索は、キーワードを使用して指定でき、クラス「sklearn.neighbors.KDTree」によって計算されます。

>  **参考資料**
>*   Bentley, J.L. (1975). [“多次元二分探索木を用いた関連検索”](http://dl.acm.org/citation.cfm?doid=361002.361007). Communications of the ACM.

### 1.6.4.3. ボールツリー

KDツリーの高次元における非効率性の問題を解決するために、<gtr gtr="90">データ構造が開発されました。KDツリーはデカルト軸（すなわち座標軸）に沿ってデータを分割するのに対し、ballツリーは一連のハイパー球に沿ってデータを分割します。この方法で構築されるツリーはKDツリーよりも多くの時間を要しますが、このデータ構造は高次元においても高い構造化データに対して非常に効果的です。


ボールツリーは、データを重心Cと半径Rを用いてノードに再帰的に分割し、各ノード内の点が![r](img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg)および![C](img/4b6d782a67ac392e97215c46b7590bf7.jpg)で定義されたハイパー球内に位置するようにします。また、*三角不等式*を用いることで、近隣検索の候補点数を減少させます。

![|x+y| \leq |x| + |y|](img/5df8f915c528f34f0ada91db5228605f.jpg)

この設定により、試験点と重心との間の単一距離計算は、ノード内のすべての点からの距離の下限と上限を決定するために十分です。ボールツリーノードの球形ジオメトリに基づいて、実際の性能は訓練データの構造に高度に依存していますが、高次元における性能は97を超えています。scikit-learnでは、ボールツリーに基づく近隣検索は94というキーワードを使用して指定でき、クラス [95] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree「sklearn.neighbors.BallTree」）で計算されます。また、ユーザーは [96] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree「sklearn.neighbors.BallTree」）クラスを使用することも可能です。

>  **参考資料**
>*   Omohundro, S.M. (1989). [「5つのボールツリー構築アルゴリズム」](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.8209). International Computer Science Institute Technical Report.

### 1.6.4.4. 最隣接アルゴリズムの選定

与えられたデータセットに対する最適なアルゴリズムの選択は複雑であり、いくつかの要素に依存しております。

*   サンプル数は102以上（すなわち100以上）であり、次元は103以上（例えば101以上）です。

    *    *強制的手法*クエリ時間は、！ [O [DN] (img/cf 8 cc 964 dfa 6 df 1 a 7473 fe 033 f 9 fb 642.jpg）において成長しています。

    *    *ボールツリー*照会時間は約 O [D log (N)] です。（img/70 abd 4 aa 320170 aa 6 dbe 8204 a 5 ed 846 e.jpg）成長

    *   KDツリーのクエリ時間の変化について、正確に記述することは困難です。コストが小さい場合（20未満）の場合、クエリの複雑さはおおよそ！ [O [D\log (N)]] (img/f211ed45608192b0763ed51c85b60811.jpg) となり、KDツリーはより効果的です。一方、コストが大きくなると、クエリの複雑さはおおよそ！ [O [DN]] (img/c5b0e465d16add1d02594ec434515c04.jpg) に近づき、ツリー構造によるオーバーヘッドが影響し、検索効率は単純な暴力的手法よりも低下します。

    小規模なデータセット（nが30未満の場合）においては、log (N) はNに相当し、暴力アルゴリズムは木構造に基づくアルゴリズムよりも効果的であると考えられます。 [sklearn.neighbors.KDTree] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree) および [sklearn.neighbors.BallTree] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) は、クエリの暴力計算におけるサンプル数の切り替えを制御するためのパラメータを提供することにより、この問題を解決します。これにより、2種類のアルゴリズムの小規模なデータセットに対する効率を暴力計算に近づけることが可能となります。

*   データ構造：データの固有次元（<gtr gtr="114">）および/またはデータの疎度（<gtr gtr="115">）。固有次元とは、データが存在する流形の次元数（<gtr gtr="112">）を指し、パラメータ空間において線形であっても非線形であっても構いません。疎度は、データがパラメータ空間に充填される程度を示すものであり、これは「疎」行列で用いられる概念とは異なります。データ行列にはゼロ項が存在しない可能性があるため、その意味においても、依然としてその疎度（<gtr gtr="113">）は「疎」とされることがあります。

    *   ＜gtr=「116」/>（暴力的なクエリ）時間はデータ構造の影響を受けません。
    *    <gtr gtr="117">および<gtr gtr="118">のデータ構造は、クエリ時間に大きな影響を及ぼします。一般的に、小次元の疎データはクエリの高速化に寄与します。KDツリーの内部表現形式はパラメータ軸に整列されているため、任意の構造化データに対しては、ボールツリーのようには表現されないことが多いです。

    機械学習において一般的に利用されるデータセットは、非常に構造化されており、ツリー構造に基づくクエリに対しても最適な特性を備えています。

*   必要な近隣数は、クエリーポイントに対して119以上である必要があります。

    Brute forceクエリ時間は、値の影響をほとんど受けません。一方で、他のクエリ時間は、特定の値の増加とともに遅くなります。これは、2つの要因によるものです。まず、特定の値が大きくなるほど、パラメータ空間での検索範囲が広がります。次に、ツリーの遍歴を行うためには、内部結果をソートする必要があります。

    木ベースのクエリにおいて、<gtr gtr="127">が<gtr gtr="128">よりも大きくなると、木の枝を剪定する能力が低下します。この状況においては、暴力的なクエリがより効果的となります。

*   クエリポイントの数に関して、Ball TreeおよびKD Treeはそれぞれ構築段階を必要とします。多数のクエリを処理する場合、この構造のコストは無視できますが、少数のクエリのみを実行する場合、構築コストは総コストの大部分を占めることになります。わずかな点を調査する場合には、木に基づく手法よりも暴力的な手法がより適していると言えます。

一般に、入力がスパース行列である場合、または特定の条件が満たされない場合には、適切な選択を行うことが求められます。具体的には、条件が満たされる場合には、選択肢の中から適切なものを選ぶことが必要です。この選択は、クエリポイントの数がトレーニングポイントの数と少なくとも同数であること、さらに特定の値がそのデフォルト値に近いという前提に基づいています。

### 1.6.4.5. <gtr gtr="「150」">の影響について

上述の通り、小規模なサンプルに対する暴力的検索は、木構造に基づく検索よりも効果的な手法である。この事実は、ball木およびKD木において、リーフノード内部で暴力的探索に切り替えることによって解釈される。この切り替えのレベルは、パラメータ<gtr gtr="151">を用いて指定することが可能である。このパラメータの選択には多くの影響がある。

より大きな<gtr gtr="152">は、より少ないノードを生成する必要があるため、迅速なツリー構築時間を実現します。

大きいまたは小さい値は、セカンダリクエリコストを引き起こす可能性があります。値が1に近づくと、エルゴードノードにかかるオーバーヘッドがクエリ時間を大幅に遅延させることがあります。また、訓練集の大きさに近づくと、クエリは本質的に暴力的なものとなります。これらの間における良好な妥協点は、当該パラメータのデフォルト値で示されます。

＜gt r=「161」/>：leaf _ sizeの増加に伴い、ストレージツリー構造に必要なメモリが減少いたします。各ノードのD次元セントロイドを格納するball treeにおいて、これは重要な要素となります。 [ `BallTree`] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree「sklearn.neighbors.BallTree」）に必要な記憶空間は、<gtr gtr="160">に訓練セットを乗算したサイズに近似されます。

 `leaf_size` 暴力的なクエリに引用する。

## 1.6.5. 最近のセントロイド分類について

この [sklearn.neighbors.NearestCentroid] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) 分類器は、各クラスをそのメンバーのセントロイドで表現するシンプルなアルゴリズムです。実際には、これはアルゴリズムのラベル更新段階に類似しています。パラメータの選択が不要であり、優れた基準分類器として機能します。しかしながら、この手法は非凸類の影響を受ける可能性があります。具体的には、クラス間に有意な異なる分散が存在する場合です。このため、この分類器はすべての次元において分散が等しいと仮定しています。この仮定を置かないより複雑な手法には、線形判別分析 ([sklearn.discriminant_analysis.LinearDiscriminantAnalysis] (https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)）や二次判別分析 ([sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis] (https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)）があります。デフォルトの [sklearn.neighbors.NearestCentroid] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) の使用例は次のとおりです。

```py
>>> from sklearn.neighbors.nearest_centroid import NearestCentroid
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> y = np.array([1, 1, 1, 2, 2, 2])
>>> clf = NearestCentroid()
>>> clf.fit(X, y)
NearestCentroid(metric='euclidean', shrink_threshold=None)
>>> print(clf.predict([[-0.8, -1]]))
[1]

```

### 1.6.5.1. 最近のセントロイドの縮小について

この [sklearn.neighbors.NearestCentroid] (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) 分類器には、nearest shrunken centroid分類器を実現するためのパラメータが存在します。具体的には、各重心の各特徴の値をその特徴のクラスの分散で割り、特徴値を縮小します。特に重要な点は、特定の固有値が0を超えている場合、それを0に設定することです。この手法により、分類器に影響を与える特徴を除去することが可能となり、ノイズ特性を除去するために有用です。

以下の例では、モデルの精度を0.81から0.82に向上させるために、わずかなshrink閾値を適用いたします。


[![nearest_centroid_1](img/27eaae520bfaa9c4bdbef494c5029741.jpg)](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_classification.html) [![nearest_centroid_2](img/a561362ff63affeb799b9d33423235a3.jpg)](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_classification.html)

>  **例を正式なトーンで書き直しました。**:
>*    <gtr gtr="172">：異なるシュリンク閾値を用いた最も近接するセントロイドによる分類の一例。

## 1.6.6 近傍成分分析

近傍成分分析（NCA）は、標準的なユークリッド距離に基づく最近傍分類の精度を向上させるための距離測定学習アルゴリズムです。このアルゴリズムは、トレーニングセットにおけるk近傍（KNN）スコアのランダム変数を直接最大化することに焦点を当てており、データの可視化や高速分類のためにデータを低次元の線形投影に適合させる能力も備えています。

[![1_6_1](img/1_6_1.png)](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_illustration.html) [![1_6_2](img/1_6_2.png)](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_illustration.html)

上記の図では、ランダムに生成されたデータセットのいくつかの点について考察しています。本研究では、特に3番目のサンプル点に関連するランダムKNN分類問題に焦点を当てました。サンプル3と他の点との間のリンクの厚さは、それらの間の距離に比例し、ランダム最隣接予測規則によってその点に割り当てられる相対的な重み（または確率）として理解することができます。元の空間においては、サンプル3は異なるクラスからのランダムな隣人が多いため、正確な分類は困難です。しかし、NCA学習による投影空間においては、唯一無視できない重みを持つランダム近傍がサンプル3と同じクラスに属し、これによりサンプル3の分類が良好であることが保証されました。詳細については、<gtr gtr="174">をご参照ください。

#### 1.6.6.1. 分類
最近傍分類器（<gtr gtr="175">）と組み合わせることで、NCAはモデルのサイズを増加させることなく、ユーザーによる微調整を要する追加のパラメータを導入することなく、多様な問題を自然に処理することができるため、効果的な分類アルゴリズムである。

NCA分類は、規模や難易度が異なるデータセットの実用化において優れた効果を示しています。線形判別分析などの関連手法と比較して、NCAはクラスの分布に対して特定の仮定を設けておりません。最近の近隣分類は、高度に不規則な意思決定境界を自然に生成する能力を有しています。

このモデルを使用して分類を行うには、<gtr gtr="176">インスタンスと<gtr gtr="177">インスタンスを結合する必要があります。<gtr gtr="178">インスタンスは適合最適変換を行い、<gtr gtr="179">インスタンスは投影空間において分類を実施します。次に、これらの二つのクラスを用いた例を示します。

```py
>>> from sklearn.neighbors import (NeighborhoodComponentsAnalysis,
... KNeighborsClassifier)
>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.pipeline import Pipeline
>>> X, y = load_iris(return_X_y=True)
>>> X_train, X_test, y_train, y_test = train_test_split(X, y,
... stratify=y, test_size=0.7, random_state=42)
>>> nca = NeighborhoodComponentsAnalysis(random_state=42)
>>> knn = KNeighborsClassifier(n_neighbors=3)
>>> nca_pipe = Pipeline([('nca', nca), ('knn', knn)])
>>> nca_pipe.fit(X_train, y_train)
Pipeline(...)
>>> print(nca_pipe.score(X_test, y_test))
0.96190476...
```

[![sphx_glr_plot_nca_classification_0011.png](img/sphx_glr_plot_nca_classification_0011.png)](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_classification.html)

[![sphx_glr_plot_nca_classification_0021.png](img/sphx_glr_plot_nca_classification_0021.png)](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_classification.html)

図においては、アヤメのデータセットにおける2つの特徴のみを用いて訓練および評価を行い、最近傍分類法および近傍成分分析に基づく分類の決定境界を示しております。これにより、直感的にその差異を観察することが可能です。

#### 1.6.6.2. ディメンションの削減
NCA (Neighborhood Component Analysis）は、監視された次元削減に利用可能です。入力データは、NCAのターゲットを最小化する方向に沿った線形サブ空間に投影されます。パラメータ `n_components` を用いることで、必要な次元数を設定することが可能です。例えば、下図は主成分分析（<gtr gtr="181">）、線形判別分析（<gtr gtr="182">）、および近傍成分分析（<gtr gtr="183">）による64の特徴と1797サンプルから構成されるデジタルデータセットの次元削減結果を示しています。このデータセットは、同一サイズのトレーニングセットとテストセットに分割され、標準化が行われます。この手法の分類精度を評価するために、各手法が得た2次元投影点に対して3−最近接分類器の精度を算出しました。各データサンプルは10のクラスのいずれかに属しています。

[![sphx_glr_plot_nca_dim_reduction_0011.png](img/sphx_glr_plot_nca_dim_reduction_0011.png)](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_dim_reduction.html)[![sphx_glr_plot_nca_dim_reduction_0021.png](img/sphx_glr_plot_nca_dim_reduction_0021.png)](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_dim_reduction.html)[![sphx_glr_plot_nca_dim_reduction_0031.png](img/sphx_glr_plot_nca_dim_reduction_0031.png)](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_dim_reduction.html)

> 申し訳ありませんが、元の内容が提供されていません。内容を再送していただければ、正式なトーンに書き直すことができます。
>>*  [近傍成分分析の有無における最近傍法の比較](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_classification.html#sphx-glr-auto-examples-neighbors-plot-nca-classification-py)[次元削減における近隣成分分析](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_dim_reduction.html#sphx-glr-auto-examples-neighbors-plot-nca-dim-reduction-py)[手書き数字に関する多様体学習：局所線形埋め込み法、アイソマップ法…](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py)

#### 