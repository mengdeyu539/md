# 1.4. サポートベクターマシン


 <gtr gtr="13">は、以下の学習アルゴリズムを監視するために利用可能です：<gtr gtr="10">、<gtr gtr="11">、<gtr gtr="12">。

サポートベクトルマシンの利点は以下の通りです。

 *   高次元空間において非常に効率的でございます。
 *   データ次元がサンプル数を上回る場合においても有効である。
 *   決定関数（支持ベクトルと呼ばれる）には、トレーニングセットのサブセットが使用されるため、メモリを効率的に活用することが可能です。
 *   汎用性：異なる核関数は特定の決定関数と一対一に対応しています。一般的なカーネルはすでに提供されており、カスタムカーネルを指定することも可能です。

サポートベクトルマシンの欠点は以下の通りです。

 *   特徴数がサンプル数よりも大幅に多い場合、核関数<gtr gtr="15">を選択する際には、オーバーフィッティングを回避することが重要であり、正則化項の役割も非常に重要です。

 *   サポートベクトルマシンは、直接的な確率推定を提供します。これらの推定は、すべて高価な5回の交差検証を用いて計算されたものであります。（詳細については[得分和概率](#1412-得分和概率)を参照してください。）

scikit-learnにおいて、サポートベクトルマシンは、密なサンプルベクトル（dense）および疎なサンプルベクトル（sparse）を出力として提供します。しかし、サポートベクトルマシンを用いて疎データを予測するためには、これらのデータに対して既に適合している必要があります。行優先記憶（C-order）を有する密なサンプルベクトルまたは疎なサンプルベクトルを使用することにより、性能を最適化することが可能です。

## 1.4.1. 分類事項

[ `SVC`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)、 [ `NuSVC`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) および [ `LinearSVC`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) は、データセットにおいて多クラス分類を実現するための手法です。


[Support Vector Classification (SVC)] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) と [Nu Support Vector Classification (NuSVC)] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) は類似の手法であるものの、異なるパラメータ設定を受け入れ、異なる数学的方程式を持っています（この部分では [参照] (<gtr gtr="34">)）。一方、 [Linear Support Vector Classification (LinearSVC)] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) は線形カーネル関数を実現する別の支持ベクトル分類器です。なお、 [LinearSVC] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) に関するキーワードは、線形であると仮定するために受け入れられません。また、 [SVC] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) と [NuSVC] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) のメンバー、例えば [参照] (<gtr gtr="33">)。

他の分類器と同様に、 [ `SVC`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)、 [ `NuSVC`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) および [ `LinearSVC`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) では、入力として2つの配列を指定する必要があります。<gtr gtr="38">サイズの配列Xは訓練サンプルとして、<gtr gtr="39">サイズの配列yはカテゴリラベル（文字列または整数）として使用されます。

```py
>>> from sklearn import svm
>>> X = [[0, 0], [1, 1]]
>>> y = [0, 1]
>>> clf = svm.SVC(gamma='scale')
>>> clf.fit(X, y)  
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
 decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
 max_iter=-1, probability=False, random_state=None, shrinking=True,
 tol=0.001, verbose=False)

```

フィット後、このモデルを用いて新たな値を予測することが可能です。

```py
>>> clf.predict([[2., 2.]])
array([1])

```

SVMの決定関数は、トレーニングセットのいくつかのサブセットに依存しており、これらはサポートベクトルと呼ばれます。これらのサポートベクトルに関する特性の一部は、<gtr gtr="40">、<gtr gtr="41">および<gtr gtr="42">で確認することができます。

```py
>>> # 获得支持向量
>>> clf.support_vectors_
array([[ 0.,  0.],
 [ 1.,  1.]])
>>> # 获得支持向量的索引
>>> clf.support_
array([0, 1]...)
>>> # 为每一个类别获得支持向量的数量
>>> clf.n_support_
array([1, 1]...)

```

### 1.4.1.1. 多重分類

[ `SVC`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) および [ `NuSVC`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) は、多クラス分類における「one-against-one」アプローチを実現する方法を示しています（Knerr et al., 1990）。もし<gtr gtr="45">がカテゴリの数を示す場合、<gtr gtr="46">分類器は再構築され、各分類器は2つのカテゴリに対してデータを訓練します。他の分類器と整合性のあるインターフェースを提供するために、 `decision_function_shape` オプションを使用することで、one-against-one分類器を集約した結果が `(n_samples, n_classes)` のサイズに基づいて決定関数となります。

```py
>>> X = [[0], [1], [2], [3]]
>>> Y = [0, 1, 2, 3]
>>> clf = svm.SVC(gamma='scale', decision_function_shape='ovo')
>>> clf.fit(X, Y)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
 decision_function_shape='ovo', degree=3, gamma='scale', kernel='rbf',
 max_iter=-1, probability=False, random_state=None, shrinking=True,
 tol=0.001, verbose=False)
>>> dec = clf.decision_function([[1]])
>>> dec.shape[1] # 4 classes: 4*3/2 = 6
6
>>> clf.decision_function_shape = "ovr"
>>> dec = clf.decision_function([[1]])
>>> dec.shape[1] # 4 classes
4

```

一方、 [ `LinearSVC`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) は、「one-vs-the-rest」という多種類戦略を実現しており、これによりn種類のモデルを訓練することが可能です。なお、クラスが2つのみの場合には、1つのモデルのみを訓練します。

```py
>>> lin_clf = svm.LinearSVC()
>>> lin_clf.fit(X, Y)
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
 intercept_scaling=1, loss='squared_hinge', max_iter=1000,
 multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
 verbose=0)
>>> dec = lin_clf.decision_function([[1]])
>>> dec.shape[1]
4

```



### 1.4.1.3. 非均等化問題

この問題においては、特定の種類または個別の例で使用されるキーワード<gtr gtr="50">および<gtr gtr="51">の重みを増加させることが望ましいと考えられます。

[ `SVC`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) ではなく、 [<gtr gtr="53">] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) の方法において、キーワード<gtr gtr="55">が生成されます。 `{class_label : value}` のような形式を持つ辞書において、値は浮動小数点数で0より大きいものであり、クラス<gtr gtr="57">のパラメータ<gtr gtr="58">を<gtr gtr="59">に設定します。

[![http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_separating_hyperplane_unbalanced_0011.png](img/9b6c97851ffb568abc5688d5c9e81800.jpg)](https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html)

[ `SVC`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), [ `NuSVC`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC), [ `SVR`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR), [ `NuSVR`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR) および [ `OneClassSVM`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM) の手法において、キーワード `sample_weight` を用いて単一の例に対する重みweightsを実現します。これらは、<gtr gtr="67">と同様に、i番目の例のパラメータ<gtr gtr="68">を<gtr gtr="69">に置き換えるものです。

[![http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_weighted_samples_0011.png](img/f298c2b42dd32bed6f02df3c6d4f7cf9.jpg)](https://scikit-learn.org/stable/auto_examples/svm/plot_weighted_samples.html)


## 1.4.2. 開期

回帰問題を解決するために、ベクトル分類を支援する方法を拡張することが可能です。この手法は、サポートベクトル回帰と呼ばれています。

ベクトル分類生成を支援するモデルは、前述の通り、モデル構築におけるコスト関数がエッジ以外のトレーニングポイントを考慮しないため、トレーニングセットのサブセットに依存します。同様に、モデル構築におけるコスト関数がモデル予測に近いトレーニングデータを無視するため、サポートベクトル回帰生成モデルもトレーニングセットのサブセットに依存します。

サポートベクトル回帰には、三つの異なる実装形態が存在します。 [ `SVR`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR)、 [ `NuSVR`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR)、および [ `LinearSVR`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR) です。線形核のみを考慮する場合、 [ `LinearSVR`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR) は [ `SVR`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR) に比べてより迅速な実装形態を提供しますが、 [<gtr gtr="75">] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR) および [ `LinearSVR`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR) に対して、 [ `NuSVR`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR) は若干異なるアイデアを実現しています。詳細については、[实现细节](#148-实现细节)を参照してください。

分類のカテゴリと同様に、fitメソッドはパラメータベクトルXを呼び出し、yについては、yが整数型ではなく浮動小数点数であることに留意します。

```py
>>> from sklearn import svm
>>> X = [[0, 0], [2, 2]]
>>> y = [0.5, 2.5]
>>> clf = svm.SVR()
>>> clf.fit(X, y)
SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto_deprecated',
 kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
>>> clf.predict([[1, 1]])
array([ 1.5])

```

申し訳ありませんが、内容が空白のため、書き直すことができません。具体的な内容を提供していただければ、正式なトーンでの書き直しを行います。

>*   [Support Vector Regression (SVR) using linear and non-linear kernels](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html)

## 1.4.3. 密度推定および異常検出（ノベルティ検出）

クラス [ `OneClassSVM`] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM）は、異常値検出のための単一クラスサポートベクターマシン（SVM）を実装しています。

OneClassSVMの説明および使用方法については、<gtr gtr="80">をご参照ください。

## 1.4.4. 複雑性

サポートベクトルマシン（SVM）は非常に強力なツールですが、トレーニングするベクトルの数が増加するにつれて、コンピュータの計算能力およびストレージの要件も急激に増大します。SVMの核心は二次計画問題（Quadratic Programming、QP）にあり、これはサポートベクトルとトレーニングデータの残りの部分を効果的に分離することにあります。実際には、データセットに関連するこのQP構文解析器は、キャッシュの有効性に基づいて、特定のスケール操作を通じて呼び出されます。データが非常に疎である場合、サンプルベクトル内のゼロでない特徴の平均数によって置き換えられます。

また、線形の場合、<gtr gtr="88">で操作される [<gtr gtr="86">] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC）アルゴリズムは、その<gtr gtr="89">に対応する [<gtr gtr="87">] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC）よりも効率的であり、数百万のサンプルや特徴に対してほぼ線形にスケールすることが可能です。


*   L1ペナルティを利用して疎解を生成します。具体的には、特徴重みのサブセットがゼロではなく、決定関数に寄与します。L1ペナルティの値を増加させることで、より複雑なモデルが生成され、より多くの特徴が選択されることになります。詳細については、 [sklearn.svm.l1_min_c] (https://scikit-learn.org/stable/modules/generated/sklearn.svm.l1_min_c.html#sklearn.svm.l1_min_c) を参照してください。このプロセスでは、すべての重みがゼロに等しい「null」モデルが生成されます。


## 1.4.6. 角関数


初期化時において、異なるカーネルは異なる関数名で呼び出されます。

```py
>>> linear_svc = svm.SVC(kernel='linear')
>>> linear_svc.kernel
'linear'
>>> rbf_svc = svm.SVC(kernel='rbf')
>>> rbf_svc.kernel
'rbf'

```

### 1.4.6.1. カスタムコア

独自のコアをカスタマイズすることや、コアとしてPython関数を利用すること、さらにはGram行列を推定することが可能です。

カスタムカーネルの分類器は、他の分類器と同様の特性を持ちますが、以下の点を除きます。

*   空間は空となり、サポートベクトルのインデックスのみがに格納されます。
*   最初のパラメータの参照（レプリカではない）が保存され、将来の参照として使用されます。配列が変化すると、予想外の結果が得られる可能性があります。

#### 1.4.6.1.1. Python関数のコアとしての利用

構築時においては、独自に定義されたカーネルを使用するために、関数を介してキーワード<gtr gtr="99">に渡すことが可能です。

カーネルは、パラメータとして2つの行列を使用する必要があります。それぞれのサイズは<gtr gtr="100">および<gtr gtr="101">であり、1つのカーネル行列を返します。その形状は<gtr gtr="102">です。

次に示すコードは、線形コアを定義し、そのコアを利用する分類器の例を構築するものです。

```py
>>> import numpy as np
>>> from sklearn import svm
>>> def my_kernel(X, Y):
...     return np.dot(X, Y.T)
...
>>> clf = svm.SVC(kernel=my_kernel)

```

>  **例を正式なトーンで書き直すと、以下のようになります：

「例」**
>*    [自定义核的SVM](https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html#sphx-glr-auto-examples-svm-plot-custom-kernel-py)

#### 1.4.6.1.2. Gram行列の利用

適応アルゴリズムにおいては、<gtr gtr="105">を設定し、XをGram行列に置換する必要があります。この場合、<gtr gtr="106">においてトレーニングベクトルとテストベクトルのカーネル値を提供することが求められます。

```py
>>> import numpy as np
>>> from sklearn import svm
>>> X = np.array([[0, 0], [1, 1]])
>>> y = [0, 1]
>>> clf = svm.SVC(kernel='precomputed')
>>> # 线性内核计算
>>> gram = np.dot(X, X.T)
>>> clf.fit(gram, y)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
 decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
 kernel='precomputed', max_iter=-1, probability=False,
 random_state=None, shrinking=True, tol=0.001, verbose=False)
>>> # 预测训练样本
>>> clf.predict(gram)
array([0, 1])

```

#### 1.4.6.1.3. RBFカーネルのパラメータ

SVMをRBFカーネルでトレーニングする際には、以下の二つのパラメータを考慮する必要があります。まず、ペナルティ係数が挙げられます。このパラメータは、すべてのSVMカーネルに共通しており、意思決定境界の単純さとトレーニングサンプルの誤分類に対する有価変換とのバランスを取る役割を果たします。小さいペナルティ係数は意思決定面をより滑らかにし、高いペナルティ係数はすべてのトレーニングサンプルを正確に分類することを目的としています。次に、単一のトレーニングサンプルが全体に与える影響の程度を定義するパラメータがあります。このパラメータが大きい場合、他のサンプルに対しても影響を及ぼすことになります。

適切な<gtr gtr="115">と<gtr gtr="116">の選択は、サポートベクターマシン（SVM）のパフォーマンスにおいて重要な役割を果たします。提案される方法は、 [<gtr gtr="117">] (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) において、＜gtr="118"/> と＜gtr="119"/> の2倍の差を設けることで、適切な値を選定することです。

