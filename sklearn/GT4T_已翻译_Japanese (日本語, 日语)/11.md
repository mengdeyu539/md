# 1.10. 決定木


 <gtr gtr="11">は、<gtr gtr="9">および<gtr gtr="10">に対する無参照監視学習手法であります。その目的は、データの特徴から簡潔な決定規則を学習し、目標変数の値を予測するモデルを構築することです。

例えば、次の画像において、決定木はif-then-elseの決定規則を用いてデータを学習し、単一の正弦波画像を推定します。決定木の深度が増すにつれて、決定規則はより複雑となり、データへの適合度が向上します。

[![http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_tree_regression_0011.png](img/f0b72920659961ba27aec1da59f3019c.jpg)](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html)

決定ツリーの利点：

*   理解と解釈を促進し、木の構造を可視化することが可能です。

*   訓練に必要なデータの量は比較的少なくて済みます。他の機械学習モデルでは、仮想変数の構築や欠損値の除去など、通常はデータの正規化が求められますが、本モデルにおいては欠損値の取り扱いがサポートされておりません。
*   トレーニング決定木におけるデータ点の数に応じて、決定木の使用に伴うオーバーヘッドは指数的に増加します。具体的には、トレーニングツリーモデルの時間的複雑さは、トレーニングに参加するデータ点の対数値に依存します。
*   数値型データおよび分類データを取り扱うことが可能です。他のテクノロジーは通常、特定の変数タイプに限定されたデータセットの分析にのみ使用されます。詳細については、アルゴリズムをご参照ください。
*   多重出力の問題に対処することが可能です。
*   ホワイトボックスモデルを適用いたします。特定の状況がこのモデルにおいて観察される場合、ブール論理を用いて容易に説明することが可能です。それに対し、ブラックボックスモデルにおける結果は、明確に説明することが困難であると言えます。
*   このモデルは、数値統計テストによって検証可能です。これにより、本モデルの信頼性を確認することができることを示しています。
*   モデル仮定の結果が実際のモデルによって提供されたデータと若干の乖離が見られる場合でも、そのパフォーマンスは依然として良好であると言えます。

決定ツリーの欠点は以下の通りです。

*   決定ツリーモデルは、データの汎化性能が低下する複雑すぎるモデルを生成しやすい傾向があります。これがいわゆるオーバーフィッティングの現象です。この問題を回避するためには、剪定やリーフノードの設定に必要な最小サンプル数、または設定可能な最大深度のようなポリシーを設けることが、最も効果的な方法となります。
*   決定ツリーは、データ内の微小な変化が全く異なるツリーの生成を引き起こす可能性があるため、不安定であることがあります。この問題は、決定ツリーの統合によって緩和されることが期待されます。
*   多方面にわたる性能最適化と単純化の要請のもと、単一の最適な決定木を学習することは通常NP困難な問題であるため、実際の決定木学習アルゴリズムは、各ノードにおいて局所的に最適な決定を行うためのヒューリスティックアルゴリズムに基づいています。このようなアルゴリズムでは、グローバル最適な決定木を生成することは保証されません。この課題は、統合学習を通じて複数の決定木を訓練することによって緩和され、これらの複数の決定木は一般的に特徴とサンプルを戻すランダムサンプリングによって生成されます。
*   意思決定木は、特定の概念を明確に表現することが難しいため、学習が困難な概念が存在します。例えば、XOR問題、パリティ問題、またはマルチプレクサの問題などが挙げられます。
*   問題において主導的なクラスが存在する場合、生成される決定ツリーに偏りが生じる可能性があります。そのため、フィッティングを行う前にデータセットのバランスを整えることを推奨いたします。

## 1.10.1. 分類

[ `DecisionTreeClassifier`] (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier「sklearn.tree.DecisionTreeClassifier」) は、データセットに対して多分類を実施するためのクラスであり、他の分類器と同様に、 [ `DecisionTreeClassifier`] (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier「sklearn.tree.DecisionTreeClassifier」) は、2つの配列（配列X）を入力として受け取り、＜gt r=「14」/＞でトレーニングサンプルを保存します。整数値の配列Yには、訓練サンプルのクラスラベルが<gtr gtr="15">で保存されます。

```py
>>> from sklearn import tree
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = tree.DecisionTreeClassifier()
>>> clf = clf.fit(X, Y)

```

合格した場合、モデルを用いてサンプルカテゴリの予測が可能となります。

```py
>>> clf.predict([[2., 2.]])
array([1])

```

葉中相における同類の訓練サンプルのスコアを基に、クラスごとの確率を予測することも可能です。

```py
>>> clf.predict_proba([[2., 2.]])
array([[ 0.,  1.]])

```

[ `DecisionTreeClassifier`] (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier「sklearn.tree.DecisionTreeClassifier」）は、二分類（ラベルが [- 1,1] の場合）および多分類（ラベルが [0,...,k-1] の場合）に利用可能です。Lrisデータセットを用いて、以下のように決定木を構築することができます。

```py
>>> from sklearn.datasets import load_iris
>>> from sklearn import tree
>>> iris = load_iris()
>>> clf = tree.DecisionTreeClassifier()
>>> clf = clf.fit(iris.data, iris.target)

```

訓練を経て、 [sklearn.tree.export_graphviz] (https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz) エクスポータは、決定ツリーをDOT形式でエクスポートします。Graphvizパッケージを管理している場合、graphvizのバイナリファイルおよびPythonパッケージをインストールするには、以下のコマンドを使用してインストールできます。

```sh
 conda install python-graphviz
```

あるいは、Graphvizプロジェクトの公式ウェブサイトからGraphvizのバイナリファイルをダウンロードし、PyPIからインストールすることも可能です。Pythonのラッパーは、<gtr gtr="20">を用いてインストールされました。次に、Irisデータセット全体でトレーニングされた上記のツリーのGraphvizエクスポート例を示します。その結果は<gtr gtr="21">に保存されます。

```py
>>> import graphviz
>>> dot_data = tree.export_graphviz(clf, out_file=None)
>>> graph = graphviz.Source(dot_data)
>>> graph.render("iris")
```

また、クラスを通じてノードのシェーディング（または回帰値）を行ったり、必要に応じて明示的な変数やクラス名を使用したりするなど、さまざまな美化をサポートしております。Jupyter Notebookは、これらのペイントノードを自動的にインラインでレンダリングする機能も備えています。

```py
>>> dot_data = tree.export_graphviz(clf, out_file=None,
...                      feature_names=iris.feature_names,  
...                      class_names=iris.target_names,  
...                      filled=True, rounded=True,  
...                      special_characters=True)  
>>> graph = graphviz.Source(dot_data)  
>>> graph
```

![http://sklearn.apachecn.org/cn/0.19.0/_images/iris.svg](img/iris.jpg)


[![http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_iris_0013.png](img/cba233fc4178da6d3fe0b177cbbb6318.jpg)](https://scikit-learn.org/stable/auto_examples/tree/plot_iris.html)

>  **例を正式なトーンで書き直すと、以下のようになります：

「例」**
>*   [Plot the decision surface of a decision tree on the iris dataset](https://scikit-learn.org/stable/auto_examples/tree/plot_iris.html#sphx-glr-auto-examples-tree-plot-iris-py)

## 1.10.2. かいき

[![http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_tree_regression_0011.png](img/f0b72920659961ba27aec1da59f3019c.jpg)](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html)

決定ツリーは、 [<gtr gtr="24">] (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor「sklearn.tree.DecisionTreeRegressor」）クラスを用いて、回帰問題を解決することも可能です。分類設定においては、フィッティング方法は配列Xおよび配列yをパラメータとして受け取り、この場合、y配列は浮動小数点値であることが期待されます。

```py
>>> from sklearn import tree
>>> X = [[0, 0], [2, 2]]
>>> y = [0.5, 2.5]
>>> clf = tree.DecisionTreeRegressor()
>>> clf = clf.fit(X, y)
>>> clf.predict([[1, 1]])
array([ 0.5])

```

>  **例を正式なトーンで書き直すと、以下のようになります：

「例」**:
>*   [Decision Tree Regression](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html#sphx-glr-auto-examples-tree-plot-tree-regression-py)

## 1.10.3. 多値出力問題

多値出力問題の一つは、Yがサイズが26の2次元配列である場合において、予測が求められる複数の出力値を有する監視学習の問題です。

出力値間に相関が存在しない場合、このタイプの問題を効果的に処理する方法として、各モデルが一つの出力に対応するn個の独立したモデルを構築し、それを用いてn個の出力を独立に予測することが考えられます。しかしながら、同一の入力に関連する出力値自体が相関している可能性があるため、一般的には、すべてのn個の出力を同時に予測する単一のモデルを構築する方がより適切であるとされています。このアプローチにより、まずモデルを構築する際のトレーニング時間が短縮されます。さらに、最終的なモデルの汎化性能も向上することが期待されます。決定木アルゴリズムにおいては、この方針を多出力問題に簡単に適用することが可能です。このためには、以下の変更が必要となります。

*   リーフには、1つのみならず複数の出力値が格納されます。
*   すべてのn個の出力に対する平均減少量を分裂基準として算出いたします。

モジュールは、<gtr gtr="27">および<gtr gtr="28">においてポリシーを実装することによって、多出力問題をサポートいたします。決定ツリーが<gtr gtr="29">サイズの出力配列Yに一致する場合、評価器：
*  ``predict``は、n _ outputの出力値を示しております。
* n _ output配列リストを``predict_proba``に出力いたします。

多出力決定ツリーを用いた回帰分析を実施いたします。この例において、入力Xは単一の実数値であり、出力YはXの正弦および余弦となります。

[![http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_tree_regression_multioutput_0011.png](img/c6b27df44672e7fa50d1d81ffbbebfbd.jpg)](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression_multioutput.html)


## 1.10.4. 複雑度の解析

全体的に、平衡二分木を構築するために要する実行時間は<gtr gtr="33">、クエリ時間は<gtr gtr="34">である。ツリーの構築アルゴリズムは平衡ツリーの生成を目指していますが、必ずしも平衡状態を維持できるわけではありません。各ノードのコストには、<gtr gtr="35">の時間的複雑性を伴う検索を通じてエントロピー低減の最大の特性を見出すことが含まれ、サブツリーが概ねバランスを保つことができると仮定されます。各ノードのコストは<gtr gtr="36">であり、決定ツリー全体の構築コストは<gtr gtr="37">となります。

Scikit-learnは、意思決定ツリーを構築するためのより効率的な手法を提供します。初期の実装では、与えられた特徴に基づく各新規分割点に沿ったクラスラベルのヒストグラム（分類用）または平均値（回帰用）を再計算します。すべてのサンプル特徴を分類し、再トレーニング時にラベルカウントを実施することにより、各ノードの複雑さを<gtr gtr="38">に低減し、総コストは<gtr gtr="39">となります。これは、ツリーベースの全てのアルゴリズムにおける改善オプションです。デフォルトでは、このアルゴリズムは勾配移動モデルに対して有効化されており、一般的にトレーニングの速度を向上させます。しかし、他の全てのアルゴリズムのデフォルト設定は無効であり、深い木を訓練する際にはトレーニング速度が低下する可能性があります。

## 1.10.5. 実際の使用方法について

*   多くの特徴を有するデータ決定ツリーにおいては、過剰適合が発生する可能性があります。高次元空間においてサンプル数が限られている場合、木構造は過剰適合しやすいため、適切なサンプルの割合と特徴の数を確保することが極めて重要です。
*   次元を事前に下げることにより（<gtr gtr="40">および<gtr gtr="41">を考慮しつつ）、ツリーのより識別性の高い特徴を見つけることが可能となります。
*   意思決定ツリーを可視化する機能により、初期木の深さとして設定された値を用いて、決定木がデータにどのように適応するかを理解することが可能です。その後、木の深さを増加させることができます。
*   フィラーツリーのサンプル数は、ツリーの追加レベルごとに増加することを考慮してください。<gtr gtr="44">を使用して、入力サイズを制御し、オーバーフィッティングを防止します。
*   リーフノード上のサンプル数は、<gtr gtr="45">および<gtr gtr="46">によって制御されます。この値が小さい場合、生成される決定ツリーは過剰適合のリスクが高まりますが、逆にこの値が大きいと決定ツリーのサンプル学習に悪影響を及ぼす可能性があります。したがって、初期値として<gtr gtr="47">を試行することが推奨されます。サンプルの変化量が大きい場合には、これらの2つのパラメータをパーセンテージとして浮動小数点数で使用することが可能です。両者の主な違いは、文献において<gtr gtr="50">がより一般的に使用されているにもかかわらず、<gtr gtr="49">はリーフノード内での最小サンプリング数を保証するのに対し、<gtr gtr="48">は任意の小さなリーフを生成することができる点にあります。
*   トレーニング前にデータセットのバランスを調整し、意思決定ツリーが主導的なクラスに偏らないようにします。クラスバランスは、各クラスから等しい数のサンプルを抽出することによって実現することができます。また、より望ましい方法として、各クラスのサンプル重みの合計を同一の値に正規化することが挙げられます。なお、重みに基づくプリトリム基準は、サンプル重みの基準を理解していないわけではなく、優性カテゴリの偏りが小さいことに留意する必要があります。
*   サンプルに重み付けが行われている場合、葉ノードがサンプルの重み付けの一部を含むことを確実にするために、重みに基づく事前トリミング基準<gtr gtr="54">を用いてツリー構造を最適化することが容易になります。
*   すべての決定ツリーの内部には<gtr gtr="55">配列が使用されており、トレーニングデータがこの形式でない場合には、データセットがコピーされます。
*   入力された行列Xがスパース行列である場合、fitメソッドを呼び出す前に行列Xをスパース形式に変換し、predictメソッドを呼び出す前に対象の行列をスパース形式にすることを推奨いたします。特徴量がほとんどのサンプルにおいてゼロの値を持つ場合、スパース行列の入力におけるトレーニング時間は、密行列に比べて数桁速くなることがございます。

## 1.10.6. 決定木アルゴリズム：ID3、C4.5、C5.0、CART

すべての種類の決定ツリーアルゴリズムには、どのような相違点が存在するのでしょうか。scikit-learnにおいては、どのようなアルゴリズムが実装されているのでしょうか。

 [ID3](https://en.wikipedia.org/wiki/ID3_algorithm)（Iterative Dichotomiser 3）は、1986年にロス・クインランによって提案されました。このアルゴリズムは、分類ターゲットの最大情報利得を生成する各ノードにおいて、貪欲な方法で分類フィーチャーを見つけるための多重ツリーを構築します。決定木は最大のサイズに成長し、その後、通常は剪定を行うことで未知のデータに対する木の汎化能力を向上させます。

C 4.5はID 3の後継アルゴリズムであり、連続属性値を離散的な間隔のグループに動的に分割することにより、特徴が明確に分類されるべきという制約を解消します。C 4.5は、トレーニングされたツリー（すなわち、ID 3アルゴリズムの出力）をif−then規則の集合に変換します。次に、各規則の精度を評価し、その適用順序を決定します。規則の正確性が変わらない場合には、木の枝を決定して解決する必要があります。

C 5.0は、Quinlanによって独自のライセンスのもとにリリースされた最新のバージョンです。本バージョンは、より少ないメモリを使用し、C 4.5に比べて小規模なルールセットを作成しつつ、より高い精度を実現しています。

CART (Classification and Regression Trees、分類と回帰ツリー）はC4.5に類似していますが、数値的な目的変数（回帰）をサポートし、規則セットを計算しない点が異なります。CARTは、各ノードにおいて最大の情報利得を生み出す特徴と閾値を用いて二分木を構築します。

scikit-learnは、CARTアルゴリズムの最適化されたバージョンを採用しています。

